{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EMR Notebook Sample\n",
    "\n",
    "This is notebook shows how EMR notebooks can make it easy to do development and analytics with PySpark.\n",
    "\n",
    "The dataset is an export of the Consumer Financial Protection Bureau database\n",
    "See https://www.consumerfinance.gov/ and specifically http://files.consumerfinance.gov/ccdb/complaints.csv.zip\n",
    "\n",
    "After the session, if you'd like to use this data please get your OWN copy by unzipping the original into S3 bucket in your own AWS account.\n",
    "\n",
    "Setup Notes\n",
    "- EMR 5.32+ requires Spark, Livy and JupyterEnterpriseGateway packages\n",
    "- To use this notebook, your cluster should have been launched with nltk-bootstrap.sh script\n",
    "- You cluster needs EBS volume increase. Suggest 15GB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_in_s3 = \"s3://heiwad-transfer/data-sets/cfpb-complaints.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direct load data from S3\n",
    "# https://stackoverflow.com/questions/40413526/reading-csv-files-with-quoted-fields-containing-embedded-commas\n",
    "df = spark.read.load(input_data_in_s3, # please get your own copy after the session\n",
    "                     format=\"csv\", sep=\",\", inferSchema=\"true\", header=\"true\", quote = '\"', escape='\"')\n",
    "\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try simple aggregation to find top companies represented in the data set\n",
    "\n",
    "res = df.groupby(\"Company\").count().orderBy('count',ascending=False)\n",
    "res.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get just the complaints column and simplify the column name\n",
    "complaints = df.select(\"Consumer complaint narrative\").withColumnRenamed(\"Consumer complaint narrative\",\"text\")\n",
    "\n",
    "# Let's sample some of the text for this column\n",
    "\n",
    "for complaint in complaints.head(8):\n",
    "    if complaint['text']:\n",
    "        print('* ' + complaint['text'] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Find out what they are complaining about\n",
    "\n",
    "Let's start with starting to count the words represented in the data. But not all words are useful so we'll filter out the common words that can be omitted from a sentence and still have it make some sense.\n",
    "\n",
    "These *stop words* will have high counts and aren't very useful for NLP so we will filter them out.\n",
    "\n",
    "The NLP library, including stop-words dictionary, was installed via bootstrap script on all nodes in the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - stopwords dictionary was installed via bootstrap script\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#--Ignore word fragments from suppressing PII in the data set\n",
    "stop_words.add('xx')\n",
    "stop_words.add('xxxx')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize via regex - very quick, could be better.\n",
    "\n",
    "import re\n",
    "\n",
    "def emitWords(row):\n",
    "    if row['text']:\n",
    "        words = []\n",
    "        tokens = re.split('\\W+',row['text'].lower())\n",
    "        for token in tokens:\n",
    "            stripped = token.strip(\"$.,1234567890\\\\/';{}~!?-\")\n",
    "            if stripped and (stripped not in stop_words):\n",
    "                words.append(stripped)\n",
    "        return words\n",
    "    else:\n",
    "        return []    \n",
    "\n",
    "# test - see how emit words parses the following sentence (code local to leader node)\n",
    "emitWords({'text':\"running. $949 . can't stop. won't stop? runners run on runs\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We can apply functions that change the shape of data by applying flatMap on the underlying rdd. This is a 'map-reduce' style operation on Spark\n",
    "counts = complaints.rdd.flatMap(emitWords) \\\n",
    "    .map(lambda word: (word, 1)) \\\n",
    "    .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "top_words = counts.top(15, key=lambda x: x[1])\n",
    "top_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EMR Notebooks can install Python packages on the leader node. This is useful for viewing or charting data.\n",
    "\n",
    "https://aws.amazon.com/blogs/big-data/install-python-libraries-on-a-running-cluster-with-emr-notebooks/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list installed packages\n",
    "sc.list_packages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.install_pypi_package(\"pandas\") \n",
    "sc.install_pypi_package(\"matplotlib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_pd = pd.DataFrame(top_words,columns=['words','count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_pd.sort_values(by='count').plot.barh(x='words', y='count', rot=0,figsize=(10,10))\n",
    "\n",
    "# Use Jupyter Magic to show the plot\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try checking the frequency of words you thought would be common below by replacing \"happy\" with anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts.filter(lambda x: \"happy\" == x[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using custom python libraries installed on the cluster\n",
    "\n",
    "The previous language model is very simple (regex). Many english words have various versions that mean more or less the same thing. If we want to break the words (run vs runs) and make sure the root is always a word this is called lemmatization. We can use language models like NLTK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to the keys to combine counts for words that mean the same thing\n",
    "\n",
    "# language model installed on cluster via bootstrap action\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_record(record):\n",
    "    (word, count) = record\n",
    "    return (lemmatizer.lemmatize(word),count)\n",
    "\n",
    "\n",
    "# See lemmatiation in action...\n",
    "words = [('go',1),('goes',1), ('run',1), ('runs',1)]\n",
    "for record in words:\n",
    "    lemma =lemmatize_record(record)\n",
    "    print (f\"{record[0]} becomes {lemma[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try lemmatization on the word counts and then compare if the top words have changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = counts.map(lemmatize_record).reduceByKey(lambda a, b: a + b)\n",
    "top_words_combined = combined.top(15, key=lambda x: x[1])\n",
    "top_words_combined_pd = pd.DataFrame(top_words_combined,columns=['words','count'])\n",
    "\n",
    "\n",
    "both_pd = pd.merge(\n",
    "    top_words_pd,\n",
    "    top_words_combined_pd,\n",
    "    how=\"left\",\n",
    "    on='words',\n",
    "    left_on=None,\n",
    "    right_on=None,\n",
    "    left_index=False,\n",
    "    right_index=False,\n",
    "    sort=False,\n",
    "    suffixes=(\"_regex\", \"_lemmatized\"),\n",
    "    copy=True,\n",
    "    indicator=False,\n",
    "    validate=None,\n",
    ")\n",
    "\n",
    "both_pd.set_index('words').sort_values(by='count_regex',ascending = True).plot.barh(figsize=(10,10))\n",
    "\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: Specify S3 bucket in your own account if you'd like to save the results back to Amazon S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Results back to S3\n",
    "\n",
    "output_s3_bucket_name= \"<bucket_name>\" # just the bucket name\n",
    "output_path=\"complaints\"\n",
    "\n",
    "s3_out = f\"s3://{s3_bucket_name}/{path}\"\n",
    "\n",
    "combined.saveAsTextFile(s3_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
